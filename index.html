<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Kyoungin Baik</title>

  <meta name="author" content="Kyoungin Baik">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/me.jpg" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Kyoungin Baik
                  </p>
                  <p>I'm an undergraduate in <a href="https://www.skku.edu/skku/index.do">Sungkyunkwan University</a>,
                    majoring in System Management Engineering & Computer Science and Engineering & Mathematics. I'm
                    interested in Reinforcement Learning and Robotics.
                    Currently, I'm working as an intern
                    at <a href="https://youngwoon.github.io/">RLLAB</a> advised by Prof. Youngwoon Lee.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:baikdenny@gmail.com">Email</a> &nbsp;/&nbsp;
                    <a href="data/KyounginBaik_CV.pdf">CV</a> &nbsp;/&nbsp;
                    <a href="https://github.com/exaflops26/">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:37%;max-width:37%">
                  <a href="images/me.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;"
                      alt="profile photo" src="images/me.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                  <p>
                    A significant amount of research is being conducted in robotics. At the end of the day, I believe
                    robots
                    will take the form of <strong>humanoids</strong>. I am progressing toward that goal step by step.
                  <p></p>

                  My current research focuses on the following key approaches:

                  <ul class="a">
                    <li> <strong>Autonomous Skill Refinement</strong>: Drawing inspiration from the natural
                      self-directed
                      learning
                      observed in humans, this approach is geared toward enabling robots to continually improve their
                      own
                      capabilities. By
                      harnessing the power of parallelized simulation platforms such as Isaac Lab, I am developing
                      methods that
                      allow
                      robots to learn from their own interactions and adapt without relying solely on explicit external
                      feedback. This
                      strategy aims to create systems that are scalable and capable of in-context learning, much like
                      the latest
                      large
                      language models.
                      <p></p>

                    <li> <strong>Advanced Visual Understanding</strong>: Effective robotic perception goes beyond simple
                      data
                      capture—it
                      requires a deep extraction of meaningful visual cues from complex sensory inputs. My work in this
                      area
                      focuses on
                      designing algorithms that learn robust visual representations through diverse simulated scenarios.
                      Utilizing
                      high-fidelity simulation tools, I seek to empower robots with a vision system that generalizes
                      well across
                      varied
                      environments, facilitating better interpretation and interaction with the world.
                      <p></p>
                    <li> <strong>Dynamic In-Field Adaptation</strong>: Real-world environments present unpredictable
                      challenges
                      such as
                      partial observability and shifting conditions. To bridge the gap between controlled simulations
                      and
                      real-life
                      applications, I am developing adaptation techniques that enable robots to adjust their behavior on
                      the
                      fly. By
                      integrating real-time feedback with in-context learning strategies, this approach ensures that
                      robotic
                      systems remain
                      robust and effective even when faced with unforeseen complexities.
                  </ul>
                  By integrating these innovative directions, my research strives to bridge the gap between
                  computational
                  intelligence
                  and cognitive adaptability, paving the way for robotic systems that are not only scalable and
                  generalized but
                  also
                  capable of continuous, self-driven evolution.
                  <p></p>
                </td>
              </tr>
              <td style="padding:20px;width:100%;vertical-align:middle;line-height:10px">
                <h2 style="padding-bottom:0px">Research Experinces and Projects</h2>
                <!-- <p style="padding-top:0px">(* denotes equal contribution)</p> -->
              </td>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:-20px">
      </tr>
      </td>
      </tr>


      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          </video></div>
          <img src='images/IMG_3228.GIF' width="160">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <span class="papertitle">Complex Long horizontal task with Online Reinforcement Learning</span>

          <br>
          <font color="purple"><strong>Byeongjin Kang</strong></font>,
          CSI Lab team members

          <br>
          <em><strong>CSI LAB research internship project</strong><em>
              <br>
              <!-- <a href="data/LLM_project_paper.pdf">project paper</a> / 
                      <a href="https://github.com/SKKUWhiteBoard/WhiteBoard_LLM">code</a> -->
              <!-- <br> -->
              <p>
                This project explored online reinforcement learning for a complex long horizontal task using a main
                board
                and cables. It involved setting up a robust learning environment, efficient data collection, and
                training
                with pixel-based RLPD. While full success in online learning was not achieved, the project provided
                valuable
                insights into the challenges of real-time adaptation and laid the groundwork for future advancements in
                online reinforcement learning.
              </p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          </video></div>
          <img src='images/max.png' width="160">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <span class="papertitle">Visual Robustness in Imitation Learning</span>

          <br>
          <font color="purple"><strong>Byeongjin Kang</strong></font>,
          CSI Lab team members

          <br>
          <em><strong>CSI LAB research internship project</strong><em>
              <br>
              <!-- <a href="data/LLM_project_paper.pdf">project paper</a> / 
                      <a href="https://github.com/SKKUWhiteBoard/WhiteBoard_LLM">code</a> -->
              <!-- <br> -->
              <p>
                This project focuses on applying action imitation learning to robotic manipulation tasks, aiming for
                visual
                robustness. It tests variations in the number of cameras and viewpoints while leveraging various vision
                techniques to ensure resilience against visual disturbances such as color changes, brightness
                variations,
                and blur in different environments. Experiments demonstrated a significant improvement in success rates,
                with the baseline model ACT achieving 10% in the training environment and 80% in different test
                environments.
              </p>
        </td>
      </tr>
      <!-- </table> -->

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          </video></div>
          <img src='images/Screen Recording 2025-03-14 at 12.02.49 AM.gif' width="160">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <span class="papertitle">Efficient Long Text Summarization Using an sLLM Pipeline</span>

          <br>
          <font color="purple"><strong>Byeongjin Kang</strong></font>,
          HoJae Kim, HunTae Kim, Joonyeol Choi, Minsu Kim, Saehun Chun
          <br>
          <!-- <em><strong>ICLR, 2025</strong><em>  -->
          <!-- <br> -->
          <a href="data/LLM_project_paper.pdf">project paper</a> /
          <a href="https://github.com/SKKUWhiteBoard/WhiteBoard_LLM">code</a>
          <br>

          <p><em>
              This project develops a compact, edge-deployable LLM for summarizing video lecture transcripts
              efficiently.
              Instead of direct fine-tuning, it uses a segmentation-based approach, dividing transcripts into
              semantically
              related segments via cosine similarity. These are clustered using methods like KNN and DBSCAN, then
              summarized
              by a specialized 500M-parameter LLM. This approach reduces computational demands while maintaining
              high-quality summaries. Evaluation with ROUGE and BERTScore shows superior performance over baseline
              models.
            </em></p>
        </td>
      </tr>
  </table>
  <tr>
    <td style="padding:20px;width:100%;vertical-align:middle" colspan="2">
      <p style="text-align:right">
        Website template from <a href="https://github.com/jonbarron/website">Jon Barron</a>.
      </p>
    </td>
  </tr>
  </td>
  </tr>
  </tbody>
  </table>
</body>

</html>